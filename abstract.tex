

Distributional Semantics (DS)  and Information Retrieval (IR)  both rely on vector-based representations.
%
DS measures the similarity between target terms
(based on a space of feature terms), and IR
measures the similarity between documents
(based on a space of terms).
%
The former is cosine-based and geometric, the latter is relevance-based and probabilistic. We explore in this paper the dualities between the apparently different
methodologies and show that 
certain equivalences
between the geometric and probabilistic methods can be established. 


The main findings include that one of the most common and successful
approaches in DS,
namely the PMI between target term and feature term, can be shown to be
closely related to the probabilistic roots of TF-IDF. 

\hrulefill

In addition to the word-based similarity, this paper expands on the relationship
between compositional DS (CDS) and semantic IR (SIR).
%
In CDS,  vectors of terms are composed to get semantic representations for phrases, clauses, and sentences of language.
In SIR, they are composed to obtain semantic representations for queries and documents.
Again here, we discover dualities between the two. 
%
It is shown that CDS's geometric sentence similarity is closely related to SIR's document-query relevance. 