\appendix

\section{PMI as a normalisation measure rather than similarity}

Difference between PMI(ft, tt) and PMI(tti, ttj)


This  shows  that PMI is  the right dual notion of IDF, but  it also shows a short coming of the usage of the PMI measure  in NLP   (looking at it from an IR point of view). Where as in IR, the PMI has a TF component ($n_L(t,d)$), in NLP, this component (i.e. its dual  FTF) is missing.   In IR terms, this means that PMI does have a  discrimination factor for the feature terms, but the impact of the  frequency between target terms and feature terms is played down.  One can try overcome this problem by  multiplying the PMI with an explicit FTF, that is form a measure such as $\FTF \cdot \textmd{PPMI}$, but this is now doing another extreme: playing up the impact of the target term-feature term frequency.  

 We conjecture that  the FTF.ITTF itself would be the more appropriate choice, hence adhering to the duality
 \[
\TF \cdot \IDF \equiv \textmd{Raw} \cdot \PMI
\]

Once PMI is computed from ITTF, other DS measures are also easily obtainable, see  table \ref{tb-ds-ir}.
 
\begin{figure*}[htb]
  \centering
  \begin{tabular}{|l|c|c|l|}
  \hline
  DS Measure  && IR Dual &\\
  \hline
 Raw Counts  &$n_{\textmd{TT}}(\featureterm,\targetterm)$ & TF &$n_{\textmd{D}}(t,d)$\\
 &&&\\
 PMI & $\log{\frac{P(\featureterm \mid \targetterm)}{P(\featureterm)}}$  &  probabilistic version of IDF  &$\log{\frac{P(t \mid q)}{P(t)}}$\\
 &&&\\
 Conditional Probability &$ \frac{P(\featureterm \mid \targetterm)}{P(\featureterm)}$ &  un-logged version of above  &$\frac{P(t\mid q)}{P(t)}$ \\
  &&&\\
  Ratio Likelihood & $\frac{P(\featureterm \mid \targetterm)}{P(\featureterm)P(\targetterm)}$ & $\frac{\textmd{un-logged version of above }}{P(q)}$ & $\frac{P(t \mid q)}{P(t) P(q)}$ \\
 &&&\\
 LMI && FTF.ITTF&\\
  \hline
  \end{tabular}
  \caption{Nothing}
  \label{tb-ds-ir}
\end{figure*}



\section{DQI}

------Thomas, is the following correct?-----
Yes.

Finally, note that in IR, PMI is a measure used to compute the degree of independence between the document and query:
\[
\log\textmd{DQI}(d,q) = \PMI(d,q)
\] 
So what DS is calculating is DQI$(\targetterm_1, \targetterm_2)$, that is,  the degree of independence between target terms.



$\PMI(\targetterm_d,\targetterm_q)$ corresponds to $\DQI(d,q)$ (ommitting details regarding the log transformation).

$\PMI(\targetterm_d,\featureterm_j)$ corresponds to
$\PMI(d,t)$; this is the LM-side of the DQI, i.e.~the decomposition of the query event.

$\PMI(\targetterm_q,\featureterm_j)$ corresponds to
$\PMI(q,t)$; this is the TF-IDF side
of the DQI, i.e.~the decomposition of the document event.

Since the PMI is symmetric (WELL, not if we decompose events!), ...





\section{PMI, IDF and ITTF (query)}
\label{sec:PMI-IDF-ITTF-(query)}


Let $d$ be a document, $q$ a query, and $t$ a term.
The logarithm of likelihood ratio of document and query is:
\[
\log\frac{P(d|q)}{P(d)} =
	\sum_t n_L(t,d) \cdot \log\frac{P(t|q)}{P(t)}
\]
Here, $n_L(t,d)=\textmd{freq}(t,d)$ is the within-document term frequency
quantification.

The likelihood ratio is a measure of (in)dependence.
The decomposition on the right side leads to the measure of independence
between term and query.
\[
\frac{P(t|q)}{P(t)} = \frac{P(t,q)}{P(t) \cdot P(q)} = \frac{P(q|t)}{P(q)}
\]
For NLP, let $\targetterm_i$ and $\targetterm_j$ be two target terms
(for which we want to establish a similarity).
$\targetterm_i$ is the ``document" and ``$\targetterm_j$" is the query.
Then:
\[
\log\frac{P(\targetterm_i|\targetterm_j)}{P(\targetterm_i)} =
	\sum_f \textmd{freq}(\featureterm,\targetterm_i) \cdot \log\frac{P(\featureterm|\targetterm_j)}{P(\featureterm)}
\]
The decomposition on the right side ...:
\[
\frac{P(\featureterm|\targetterm_j)}{P(\featureterm)} =
\frac{P(\textmd{feature}|\textmd{target})}{P(\textmd{feature})} =
\frac{P(\textmd{target}|\textmd{feature})}{P(\textmd{target})}
\]

The problem is that the term probability (the feature probability)
is based on the ``total" occurrence, whereas the IDF is based
on counting documents.
In other words,
the feature probability (for IR, term probability) is based on
the occurrence of the event:
$P(\textmd{feature term}):=P_{\textmd{L}}(\textmd{feature term})$
is based on the Locations (the number of times the feature occurs), so is
$P(\targetterm):=P_{\textmd{L}}(\targetterm)$.

Transformation to the ``document" (the target term) via Poisson bridge.
...

For IR:
\[
P(t) = P(t|q) \cdot P_{\textmd{D}}(t)
\]
\[
P(t|q) = \avgtf(t) / \avgdl
\]
Therefore:
\[
\log\frac{P(t|q)}{P(t)} = \log\frac{1}{P_{\textmd{D}}(t)}
\]

For NLP/PMI:
\[
P(\featureterm) = P(\featureterm|\targetterm) \cdot P_\textmd{TT}(\featureterm)
\]
\begin{equation}
\label{eq:pmi-log}
\log\frac{P(\featureterm|\targetterm_j)}{P(\featureterm)} = \log\frac{1}{P_\textmd{TT}(\featureterm)}
\end{equation}

This shows that the dual to TF.IDF in DS is Raw.PMI, 
\[
\TF \cdot \IDF \equiv \textmd{Raw} \cdot \PMI
\]
Herein, the log expression expresses how discriminative the feature is. 

Once PMI is computed from ITTF, other DS measures are also easily obtainable, see  table \ref{tb-ds-ir}.
 
\begin{figure*}[htb]
  \centering
  \begin{tabular}{|l|c|c|l|}
  \hline
  DS Measure  && IR Dual &\\
  \hline
 Raw Counts  &$n_{\textmd{TT}}(\featureterm,\targetterm)$ & TF &$n_{\textmd{D}}(t,d)$\\
 &&&\\
 PMI & $\log{\frac{P(\featureterm \mid \targetterm)}{P(\featureterm)}}$  &  probabilistic version of IDF  &$\log{\frac{P(t \mid q)}{P(t)}}$\\
 &&&\\
 Conditional Probability &$ \frac{P(\featureterm \mid \targetterm)}{P(\featureterm)}$ &  un-logged version of above  &$\frac{P(t\mid q)}{P(t)}$ \\
  &&&\\
  Ratio Likelihood & $\frac{P(\featureterm \mid \targetterm)}{P(\featureterm)P(\targetterm)}$ & $\frac{\textmd{un-logged version of above }}{P(q)}$ & $\frac{P(t \mid q)}{P(t) P(q)}$ \\
 &&&\\
 LMI && FTF.ITTF&\\
  \hline
  \end{tabular}
  \caption{Nothing}
  \label{tb-ds-ir}
\end{figure*}


This  shows  that PMI is  the right dual notion of IDF, but  it also shows a short coming of PMI   (looking at it from an IR point of view): the FTF component of PMI is also  logarithmic. In IR terms, this means that PMI does have a  discrimination factor for the feature terms, but the impact of the  frequency between target terms and feature terms is played down. 

One can try overcome this problem by  multiplying the PMI with an explicit FTF, that is form a measure such as $\FTF \cdot \textmd{PPMI}$, but this is now doing another extreme: playing up the impact of the target term-feature term frequency.  

But we conjecture that  the FTF.ITTF itself would be the more appropriate choice.   This measure will allow us to reason about the similarity between two terms by dualizing from  the similarity between document and query. That is, we can define:  $\textmd{sim}(\targetterm_i,\targetterm_j)$ is high if we have:
\begin{enumerate}
\item Many feature terms occur $k$-close-to  subject, verb, and object.
\item The feature terms are rare.
\end{enumerate}

------Thomas, is the following correct?-----
Yes.

Finally, note that in IR, PMI is a measure used to compute the degree of independence between the document and query:
\[
\log\textmd{DQI}(d,q) = \PMI(d,q)
\] 
So what DS is calculating is DQI$(\targetterm_1, \targetterm_2)$, that is,  the degree of independence between target terms.



$\PMI(\targetterm_d,\targetterm_q)$ corresponds to $\DQI(d,q)$ (ommitting details regarding the log transformation).

$\PMI(\targetterm_d,\featureterm_j)$ corresponds to
$\PMI(d,t)$; this is the LM-side of the DQI, i.e.~the decomposition of the query event.

$\PMI(\targetterm_q,\featureterm_j)$ corresponds to
$\PMI(q,t)$; this is the TF-IDF side
of the DQI, i.e.~the decomposition of the document event.

Since the PMI is symmetric (WELL, not if we decompose events!), ...




\section{Deriving the DS phrase Similarity in IR}
\label{sec:DS-Phrase-Similarity-vs-XF-IDF}


In this section we show how the degree of similarity of DS can be derived from a generalised notion of inner product in IR and how this yields a symmetric version of the degree of implication in IR. 
 
In order to be able to measure the DS similarity of a document and a query, one has to form their inner product:
\[
\vec{d} \cdot \vec{q}
\]
where the vectors of document and query are defined (for a space of dimension three) as follows:
\[
\vec{d} =
	\left(
	\begin{matrix}
	P(t_1|d)\\
	P(t_2|d)\\
	P(t_3|d)\\
	\end{matrix}
	\right),
\qquad
\vec{q} =
	\left(
	\begin{matrix}
	P(q|t_1)\\
	P(q|t_2)\\
	P(q|t_3)\\
	\end{matrix}
	\right)
\]
Recall the degree of implication equation:
\[
P(q|d) = \sum_t P(q|t) \cdot P(t|d)
\]
Rewrite it as follows:
\[
P(q|d) = \frac{1}{P(d)} \cdot \sum_t P(q|t) \cdot P(d|t) \cdot P(t)
\]

The reformulation helps
to make $\vec{d}$ and $\vec{q}$ having the same semantics:
\[
\vec{d} =
	\left(
	\begin{matrix}
	P(d|t_1)\\
	P(d|t_2)\\
	P(d|t_3)\\
	\end{matrix}
	\right)
\]
The rewriting is obtainable by using a generalised inner product:
\[
\vec{d}^T \cdot G \cdot \vec{q}
\]
where the  $G$ operator is defined as follows:
\[
G=
\left[
\begin{matrix}
P(t_1) \\
0 & P(t_2) \\
0 & 0 & P(t_3) \\
\end{matrix}
\right]
\]


As a result  we obtain a symmetric version of implication, that is we have:
\[
P(d \to q) \equiv P(q \to d) \equiv \vec{d}^T \cdot G \cdot \vec{q}
\] 
This is the same as the inner product, hence dualists the similarity measure of DS. 

\section{NLP vs IR}

\begin{table*}[htbp]
\begin{tabularx}{\textwidth}{l|XX}
& NLP & IR\\
\hline
co-occurrence
&
        co-occurrence between the semantic symbol (target word) and feature words
        &
        co-occurrence between the semantic symbols (words) themselves
        \\
        \hline
representation of words
        & distributional & symbolic\\
        \hline
single vs set & similarity between two
    \emph{single phrases}
    & relationship (implication, entailment) between two
    \emph{sets of phrases}\\

symmetric yes/no &
    similarity is a symmetric function &
relationship between sets is not symmetric; moreover, the phrase-based score is not necessarily symmetric\\
\hline
similar/relevant
& phrase $t_i$ is similar to phrase $t_j$ &
document~$d$ (source) is relevant with respect to query~$q$ (target)\\
\hline
scores
& the similarity score is estimated
based on the distance/angle between distributional vectors
& the relevance score is estimated
based on the retrieval model that computes the implication between the set of document and the set of query propositions\\
probabilistic semantics
& (in)dependence between target word and feature word: $\frac{P(w_t,w_f)}{P(w_t) \cdot P(w_f)}$
& (in)dependent between document and query:
$\frac{P(d,q)}{P(d) \cdot P(q)}$\\
\hline
& for this work: virtual query has exactly one proposition; virtual document has exactly one condition; therefore, the similarity score $\textmd{sim}(\textmd{phrase1},\textmd{phrase2})$ can be compared to the retrieval score
$\textmd{RSV}(\textmd{document: set of phrases},\textmd{query: set of phrases})$.

TODO: how to get this over two columns
\\
        \hline
\end{tabularx}
\end{table*}

\section{data}

\begin{verbatim}
Target,Context,"N5(t, c)",N(t),N(c),P(c),P5(c|t),R5,PMI5,PPMI5,SIM_ppmi,SIM_ratio
man,woman,72594,955363,576966

0.00025625057145412807,0.07598577713392711,296.5292007066976,5.692145698265373,5.692145698265373

0.9377142577638676,0.7264246690255856
\end{verbatim}


\section{Why the independence measure is best}

Multiplication of three vectors: s, v and o.
\[
\frac{
P(c|s) \cdot P(c|v) \cdot P(c|o)}{
P(c) \cdot P(c) \cdot P(c)} =
\frac{
P(s|c) \cdot P(v|c) \cdot P(o|c)
}{
P(s) \cdot P(v) \cdot P(o)
}
\]
Let $t_i$ and $t_j$ be two propositions
(e.g.~s-v-o).

$\textmd{sim}(t_i,t_j)$ is high if
\begin{enumerate}
\item many features occur within the context of subject, verb, and object
\item the features are rare
\end{enumerate}

cos: corresponds to sum over independence measure.
Therefore, pmi is the correct model.
Corresponds to
\[
\sum_c \log(...) = \log \prod_c(...)
\]



Does not work for different
spaces for nouns and verbs?

The multiplication yields a probability
for the subj-verb-obj sequence where all
components are independent of each other.

Multiplication of two conditional probs means then what?
Effect of L2 norm?


Relationship to LM - there should be an obvious one.

\section{SVO Optimial Weighting}

\begin{verbatim}
w_space0{
0.40 (subj);
0.30 (verb);
0.30 (obj);
# corr: R5: 76.7
}
w_space1{
0.45 (subj);
0.35 (verb);
0.20 (obj);
# corr: R5: 75.x
}

w_space2{
0.35 (subj);
0.40 (verb);
0.25 (obj);
# corr: R5: 75.0
}
\end{verbatim}


\begin{tabular}{lr}
\toprule
{} &         Symbolic to Mixture relative improvement \\
\midrule
BNC N5      &  0.060823 \\
BNC P5      &  0.111693 \\
BNC PPMI5   &  0.075668 \\
BNC R5      &  0.035765 \\
ukWaC N5    &  0.075501 \\
ukWaC P5    &  0.203443 \\
ukWaC PPMI5 &  0.049865 \\
ukWaC R5    &  0.023468 \\
\bottomrule
\end{tabular}

\section{Decomposing Similarity}
Decompsoed additive baseline similarity
\[
\frac{cos(sbj1,sbj2) + cos(vrb1,vrb2)) + cos(obj1,obj2)}{3}
\]

Can any of the NLP sentence similarities decompose? It does not seem so, for example
\begin{eqnarray*}
cos(sbj1+verb1,sbj2+vrb2) &\neq& cos(sbj1, sbj2) + cos(vrb1,vrb2) \\
cos(sbj1\odot verb1,sbj2+vrb2) &\neq& cos(sbj1, sbj2) \odot cos(vrb1,vrb2)
\end{eqnarray*}
For kronecker composition has to move between vector of different tensor rank, i.e. either from $V$ to $V \otimes V$ or from the latter to the former. Diagonalization might be helpful here, also convolution kernel might help. 

In general, may be the sentence cosines, do doecompsoe but not in a direct way as above, that is when left and right have the same operation. The operations might vary from lef to right, or the ratios might be preserved etc.


\section{Why the independence measure is best}

Multiplication of three vectors: s, v and o.
\[
\frac{
P(c|s) \cdot P(c|v) \cdot P(c|o)}{
P(c) \cdot P(c) \cdot P(c)} =
\frac{
P(s|c) \cdot P(v|c) \cdot P(o|c)
}{
P(s) \cdot P(v) \cdot P(o)
}
\]



\section{Decomposing Similarity}
Decompsoed additive baseline similarity
\[
\frac{cos(sbj1,sbj2) + cos(vrb1,vrb2)) + cos(obj1,obj2)}{3}
\]

Can any of the NLP sentence similarities decompose? It does not seem so, for example
\begin{eqnarray*}
cos(sbj1+verb1,sbj2+vrb2) &\neq& cos(sbj1, sbj2) + cos(vrb1,vrb2) \\
cos(sbj1\odot verb1,sbj2+vrb2) &\neq& cos(sbj1, sbj2) \odot cos(vrb1,vrb2)
\end{eqnarray*}
For kronecker composition has to move between vector of different tensor rank, i.e. either from $V$ to $V \otimes V$ or from the latter to the former. Diagonalization might be helpful here, also convolution kernel might help. 

In general, may be the sentence cosines, do doecompsoe but not in a direct way as above, that is when left and right have the same operation. The operations might vary from lef to right, or the ratios might be preserved etc. 



\section{cosines again}

\begin{eqnarray}
\cos(angle(s\odot v\odot o, ...) &=&  1/ \sum P(s,v,o|c) \cdot P(c)\\
\cos(angle(s+v+o, ...) &=& \sum (P(s|c)  \cdot  ... + P(v|c) ... + ....)  \cdot  ....\\
\cos(angle(s+v+o, ...) &=&  \sum (\log{\Pi_{x in s,v,o} (P(x,c)/P(x) \cdot P(c)) ...}\\
\end{eqnarray}

\section{various log measures}

\begin{eqnarray}
\textmd{NLogP}&   N(t,c)  \cdot \log{1}{P(c)}\\
\textmd{LogNLogP}&  log(N(t,c)+1) \cdot \log{1}{P(c)}\\
\textmd{PExpN}&    P(c)^{-N(t,c)}\\
\textmd{PExpLogN}& P(c)^{-\log{N(t,c)+1}}\\
\end{eqnarray}

\begin{table*}[htbp]
\begin{tabularx}{\textwidth}{l|XX}
& NLP & IR\\
\hline
co-occurrence
&
        co-occurrence between the semantic symbol (target word) and feature words
        &
        co-occurrence between the semantic symbols (words) themselves
        \\
        \hline
representation of words
        & distributional & symbolic\\
        \hline
single vs set & similarity between two
    \emph{single phrases}
    & relationship (implication, entailment) between two
    \emph{sets of phrases}\\

symmetric yes/no &
    similarity is a symmetric function &
relationship between sets is not symmetric; moreover, the phrase-based score is not necessarily symmetric\\
\hline
similar/relevant
& phrase $t_i$ is similar to phrase $t_j$ &
document~$d$ (source) is relevant with respect to query~$q$ (target)\\
\hline
scores
& the similarity score is estimated
based on the distance/angle between distributional vectors
& the relevance score is estimated
based on the retrieval model that computes the implication between the set of document and the set of query propositions\\
probabilistic semantics
& (in)dependence between target word and feature word: $\frac{P(w_t,w_f)}{P(w_t) \cdot P(w_f)}$
& (in)dependent between document and query:
$\frac{P(d,q)}{P(d) \cdot P(q)}$\\
\hline
& for this work: virtual query has exactly one proposition; virtual document has exactly one condition; therefore, the similarity score $\textmd{sim}(\textmd{phrase1},\textmd{phrase2})$ can be compared to the retrieval score
$\textmd{RSV}(\textmd{document: set of phrases},\textmd{query: set of phrases})$.

TODO: how to get this over two columns
\\
        \hline
\end{tabularx}
\end{table*}


\section{Decomposition Identities}
{Due to  logarithm identities such as  $\log(a\cdot b) = \log(a) + \log(b)$, we get identities  between the intra-word-level composition methods and the inter-word-level weighting measures, e.g. : 

\begin{proposition}
\[
\ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\PMI}^{+} = \log(\ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\Ratio}^{\odot})
\]
\end{proposition}
\begin{proof}
\begin{eqnarray*}
\ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\PMI}^{+} &=& \ov{\sbj}_{\PMI} + \ov{\vrb}_{\PMI} + \ov{\obj}_{\PMI} \\
&=&  \log\frac{P(\featureterm_j \mid \sbj)}{P(\featureterm_j)} + \log\frac{P(\featureterm_j \mid \vrb)}{P(\featureterm_j)} + \log\frac{P(\featureterm_j \mid \obj)}{P(\featureterm_j)}\\
&=& \log(\frac{P(\featureterm_j \mid \sbj)}{P(\featureterm_j)} \cdot \frac{P(\featureterm_j \mid \vrb)}{P(\featureterm_j)} \cdot \frac{P(\featureterm_j \mid \obj)}{P(\featureterm_j)})\\
&=& \log (\ov{\sbj}_{\Ratio} \odot \ov{\vrb}_{\Ratio} \odot \ov{\obj}_{\Ratio})
\end{eqnarray*}
\end{proof}

\begin{proposition}
\label{prop:TF-ITTF}
\[
\ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\FTF.\ITTF}^{+} = \ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\textmd{Raw}}^{+} \cdot  \ITTF (\textmd{z}, \TT)
\]
for $\textmd{z} \in \{\sbj, \vrb, \obj\}$. 
\end{proposition}
\begin{proof}
 Note:  $(i)$ ITTF is invariant to  its first argument, since:
 \[
 \log\frac{1}{P_{\TT}(\featureterm)} =  \ITTF(\sbj, \TT)  = \ITTF(\vrb, \TT) = \ITTF (\obj, \TT) 
\]
and $(ii)$ FTF is the same as raw counts, that is
 \[
 \FTF(\textmd{z},\featureterm) = \ov{\textmd{z}}_{\textmd{Raw}}
 \]
 The proposed identity is then obtained as follows:
\begin{align*}
&\ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\FTF.\ITTF}^{+} =& \\
&( \FTF(\sbj, \featureterm) + 
\FTF(\vrb, \featureterm)  + 
\FTF(\obj, \featureterm) ) \cdot  \log\frac{1}{P_{\TT}(\featureterm)}
=& \\
&(\sum_{z \in \{\sbj, \vrb, \obj\}} \FTF(z, \featureterm)) \cdot \ITTF(z, \TT) =& \\
&(\ov{\sbj}_{\textmd{Raw}} + \ov{\vrb}_{\textmd{Raw}} + \ov{\obj}_{\textmd{Raw}})  \cdot  
\ITTF(z, \TT) &
\end{align*}
for $\textmd{z} \in \{\sbj, \vrb, \obj\}$
\end{proof}

The above property is invariant under the composition method.  Similar identities hold  for the point wise multiplication and tensor contraction composition methods. That is:

\medskip
Proposition \ref{prop:TF-ITTF} (Continued).
\begin{eqnarray*}
\ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\FTF.\ITTF}^{\odot} &=& \ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\textmd{Raw}}^{\odot} \cdot  \ITTF (\textmd{z}, \TT)\\
\ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\FTF.\ITTF}^{\times} &=& \ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\textmd{Raw}}^{\times} \cdot  \ITTF (\textmd{z}, \TT)
\end{eqnarray*}
for $\textmd{z} \in \{\sbj, \vrb, \obj\}$. 




\begin{proposition}
\[
\ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\PMI}^{\times}  =  \log(\ov{\sbj}_{\Ratio}) \times (\log(\ov{\vrb}_{\Ratio}) \times \log(\ov{\obj}_{\Ratio}))
\]
\end{proposition}

\begin{proof}
For simplicity suppose $\ov{\vrb}_x$ is a $2 \times 2 \times 2$ cube with coordinates $v_{ijk}$ at depth $i$, row $j$ and column $k$. Suppose further that  $\ov{\sbj}_{\Ratio}$ and $\ov{\obj}_{\Ratio}$ are $2 \times 1$ and $1 \times 2$ vectors with coordinates $s_{ij}$ and $o_{ij}$, respectively,  at row $i$ and column $j$. Holding the depth dimension to be fixed for $i = 1$ for the moment,  tensor contraction $\ov{\vrb}_{\PMI} \times \ov{\obj}_{\PMI}$ can be denoted as follows
\begin{eqnarray*}
&\left(\begin{array}{cc}
\log v_{111} & \log v_{112}\\
\log v_{121} & \log v_{122}
\end{array}\right)
 \times 
\left( \begin{array}{c}
\log o_{11}\\
\log o_{21}
\end{array}\right)&\\
&=
\left(\begin{array}{cc}
\log v_{111} \log o_{11} + \log v_{112} \log o_{21}\\
\log v_{121} \log o_{11} + \log v_{122} \log o_{21}
\end{array}\right)&
\end{eqnarray*}
It is easily established that the above is equal to the following
\[
\log \left ( \begin{array}{cc}
v_{111} & v_{112}\\
v_{121} & v_{122}
\end{array}
\right)
\times
\log \left ( \begin{array}{c}
o_{11} \\ o_{21}
\end{array}\right)
\]
Further matrix multiplying this with the vector of the subject will result in a second similar identity, which can be put together with the first one above to obtain the proposition. 
\end{proof}
}


\section{Attic}
\subsection{Extending with probability distribution of POS}
The impact of a phrase on the retrieval score is given by the XF-IDF
weight of a phrase.
%
Let $(s,v,o)$ be a subj-verb-obj phrase.

Let $\textmd{type}(x,(s,v,o))$ be a function that returns the type (PartOfSpeech) of the component
of the phrase, i.e.~$\textmd{type}(s,(s,v,o))=\textmd{subject}$.
For example:
\[
\textmd{type}(\textmd{sailor},\textmd{(sailor,see,dolphin)})=\textmd{subject}
\]
In the following, we employ $\varphi=(s,v,o)$ to stand for a proposition, and $x\in\{s,v,o\}$ to be one of the components of the proposition.

Then:
\begin{equation}
\label{eq:xf-idf}
w_\textmd{XF-IDF}((s,v,o),d) :=
\sum_{x \in \{s,v,o\}} \XF(x,d) \cdot 
\IDF(x) \cdot P(\textmd{type}(x),(s,v,o))
\end{equation}
\


$P$ is a probability distribution over the types (POS), that is we have:
\[
\sum_{\textmd{type}} P(\textmd{type})  \ = \ 1
\]

\subsection{how to do tensors in IR}
At this point we have established that  FTF.ITTF weighting is the right IR dual of TF.IDF. The interesting question now regards the  IR duals of  composition operators of DS:  what would  be the IR duals of point wise multiplication and tensor contraction (with FTF.ITTF as weighting)?  One can indeed easily dualise the point wise multiplication composition in IR, again for $P(\textmd{type}(x,(s,v,o))) = 1$, as follows:
\[
\Pi_{x \in \{s,v,o\}} \TF(x,d) \cdot \IDF(x) \cdot P(\textmd{type}(x,(s,v,o)))\]
For tensor contraction, one has to assign a tensor to a verb; this can be done thanks to the $P(\textmd{type}(x,(s,v,o)))$  function. One can make it assign  1 to s and o, and assign $I$ to  v, where $I$ the unit cube with 1's on the diagonal and 0's every where else, defined as follows
 \[
 I = \sum_{ijk} \featureterm_i \otimes \featureterm_i \otimes \featureterm_i
 \]
 This will make the TF.IDF values sit on the diagonal of a cube.  Hence $P$ is defined as follows 
\[
\overline{P} := P(\textmd{type}(x,(\sbj,\vrb,\obj))) =  \begin{cases}
1 & x = \sbj \\
1 & x = \obj\\
I & x = \vrb
\end{cases}
\]
But we still have to denote the tensor contraction operation. We do so by parametrizing the $w_\textmd{XF-IDF}$ as follows
\[
\begin{cases}
w_\textmd{XF-IDF}^+ & \mbox{the usual IR composition}\\
w_\textmd{XF-IDF}^{\odot} &   \mbox{point wise multiplication instead of summation}\\
w_\textmd{XF-IDF}^{\times} &   \mbox{tensor contraction}
\end{cases}
\] 
 Thus we can fill in the following table:
 \begin{center}
 \begin{tabular}{c|c}
 IR & DS\\
 \hline
 \hline
 &\\
$w_\textmd{XF-IDF}^+((s,v,o), 1)$ &$ \ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\FTF.\ITTF}^{+}$ \\
&\\
$w_\textmd{XF-IDF}^{\odot}((s,v,o), 1)$ & $ \ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\FTF.\ITTF}^{\odot}$ \\
&\\
$w_a\textmd{XF-IDF}^{\times}((s,v,o), \overline{P})$ & $ \ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\FTF.\ITTF}^{\times}$ 
 \end{tabular}
 \end{center}
 

 But would these be useful in any  IR tasks? Tensor contraction has caused speed ups in some DS tasks, would this happen in IR as well? From the other side, one can ask whether playing with the value of $P(\textmd{type}(x,(s,v,o)))$ in $w_\textmd{XF-IDF}((s,v,o), f(P))$  get us anything interesting in DS? Our conjecture here is that one will get the $n$-grams of language models. 
 

\subsection{The P in XF-IDF}
XF-IDF is related to the additive phrase model of DS. Unfolding its definition from  equation \ref{eq:xf-idf} for  $P(\textmd{type}(x,(s,v,o))) = 1$  yields
 \[ 
 \TF(\sbj,d) \IDF(\sbj) P(s) +  \TF(\vrb,d) \IDF(\vrb) P(v) +  \TF(\obj,d) \IDF(\obj) P(o)
 \] 
A  connection to  the NLP vector model of phrases is immediately realized by observing that  if we  multiply the  FTF.ITTF coordinates of each word vector with the probability that whether that word is either  a subject, a verb, or an object, and apply the additive model of composition, then we obtain the XF-IDF model: 
 \[
w_\textmd{XF-IDF}((s,v,o)) \equiv  \ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\FTF.\ITTF.P}^{+} 
 \]
A second possibility is to assume that P is a  uniform distribution, use the FTF-ITTF as before, but then take its average over the number of words in the phrases. So we have
\[
P(s) = P(v) = P(o) = 1/3
\]
hence, 
 \begin{align*}
&w_\textmd{XF-IDF}((s,v,o)) = \\
&(\TF(\sbj,d) \IDF(\sbj) +  \TF(\vrb,d) \IDF(\vrb)  +  \TF(\obj,d) \IDF(\obj))/3
\end{align*}
Denoting the average operation by Avg, we obtain 
\[
w_\textmd{XF-IDF}((s,v,o)) = \ov{\sbj\text{-}\vrb\text{-}\obj}\ _{\FTF.\ITTF}^{\textmd{Avg}}
 \]

Both of these options, i.e. multiplying the word vector coordinates with the probability that they are in a certain grammatical role, or taking the overall average  are  possible extensions to the NLP vector models and worth experimenting with. 


