\section{Introduction \& Motivations}


The term TF-IDF is often used in DS as a weighting scheme when it comes to normalizing  the number of times a target term occurred close to (e.g. in a window of $k$-words) a feature. The notion of TF-IDF comes from and is a well-known notion, however, this known notion is not what is used as TF-IDF in DS. Here ,  most known and worked-with of  is point wise mutual information or  PMI, defined as follows:
\[
 PMI (f, t) =  \log{{P(f \mid t)} \over{P(f)}}
 \]


In this paper first we solve the mystery of why  the above (and its brothesr and sisters: LMI, Likelihood Ration, and PPMI) may be called TF-IDF or be a TF-IDF-based measure. We discover new relationships between IR's TF-IDF and DS's PMI. It turns out that the IR notion of `relevance'  is also closely related to the DS notion of `cosine similarity'. 

Then, from single terms we move to phrases and show how DS notion of  compositionality relates to a similar notion in  IR's, used in semantic IR and applied to propositions. 


\hrulefill

Depending on the co-occurrence quantification, and depending on the composition for s-v-o, for some expressions, one  gets equivalences between geometric and probabilistic (and information-theoretic) expressions.


\cite{Wong/Yao:95}:
\[
\vec{d} \cdot \vec{q}
\]
\[
\vec{d} =
	\left(
	\begin{matrix}
	P(t_1|d)\\
	P(t_2|d)\\
	P(t_3|d)\\
	\end{matrix}
	\right),
\qquad
\vec{q} =
	\left(
	\begin{matrix}
	P(q|t_1)\\
	P(q|t_2)\\
	P(q|t_3)\\
	\end{matrix}
	\right)
\]
\[
P(q|d) = \sum_t P(q|t) \cdot P(t|d)
\]
Rewrite as follows:
\[
P(q|d) = \frac{1}{P(d)} \cdot \sum_t P(q|t) \cdot P(d|t) \cdot P(t)
\]

The reformulation helps
to make $\vec{d}$ and $\vec{q}$ having the same semantics:
\[
\vec{d} =
	\left(
	\begin{matrix}
	P(d|t_1)\\
	P(d|t_2)\\
	P(d|t_3)\\
	\end{matrix}
	\right)
\]
Generalised inner product:
\[
\vec{d}^T \cdot G \cdot \vec{q}
\]
\[
G=
\left[
\begin{matrix}
P(t_1) \\
0 & P(t_2) \\
0 & 0 & P(t_3) \\
\end{matrix}
\right]
\]


%The starting point of this approach is the  window-based  vector space models of natural language \cite{Schutze}, also referred to by \emph{distributional} models of meaning. These are inspired by  the vector space models of  Information Retrieval \cite{Salton}. In IR, terms are vectors whose coordinates are their degree of appearance in documents, and symmetrically documents can also be seen as vectors whose coordinates are the degree of appearance of words in them. The vector spaces in which meanings of words live have documents as basis vectors. These models have been used to retrieve documents that are relevant to a query, seen as a collection of terms (words). The window-based vector space models are slightly different, herein,  basis vectors are other words (sometimes also referred to by context words or features). In such models, one fixes a neighbourhood window of $k$ words (e.g. $k = 5$) then computes the degree of co-occurrence of a word with other words within that window. Hence, each words will have a vectors and the distance between the word vectors can be used to reason about their  similarity of meanings. The origins of this work and its application to word meaning, similarity, and disambiguation  goes back to the work of Schutze. Later various different normalisation schemes and distances were established and experimented with and it turned out the the cosine of the angle between the vectors is a good measure of similarity, e.g. see \cite{BullerinaLevy,Curran}. 





\section{Single Terms}


\subsection{InvFeatureTermFreq or InvTargetTermFreq}




 

In IR, the main measure of retrieval is TF-IDF. One is given a  single term $t$ (in DS known as a `word'),  a document $d$, and a set of documents $D = \{d_1, d_2, \cdots, d_N\}$. Then one computes  the frequency  of   $t$  in $d$, multiplied by  the logarithm of the  total number of documents $n$ divided by the number of documents with  term $t$ in them (for the remainder of this section `term' stands for `single term'). The most traditional way of writing this formula is as  follows:

\begin{equation}
\label{tf-idf}
\TF(t,d) \cdot \IDF(t, D) = freq(t, d) \cdot \log{{N} \over {\mid \{d \in D \mid t \in d\}\mid}}
\end{equation}

The above formula can be looked at in a probabilities way. One has to have an appropriate event space.  Using notation from   \cite{ThomasBook} this is possible;  first assume:
\begin{itemize}
\item $N_D$: total number of documents in $D$, i.e.   $\mid D \mid$
\item $n_D(t)$: number of documents in which $t$ occurs.
\item $n_L(t)$: number of locations in which  term $t$ occurs.
\item $N_T$: total number of terms.
\item $N_L$: total number of locations.
\end{itemize}

The  TF-IDF formula becomes equivalent to the following 
\[
 freq(t, d) \cdot \log{\frac{1}{P(t)}}
\]
where $P(t)$  is the probability of picking a document with the term $t$ in it, that is 
\[
P(t) = \frac{n_D(t)}{N_D} 
\]
This probability is generalised using locations:  
\[
P(t) = \frac{n_L(t)}{N_L}
\] 
where previously  `document' was instantiated for `location'. 


\medskip
In DS, given is a corpus of text in which one works with the frequency of co-occurrence of a term with  features, in a window of length $k$.  Suppose we have a set of  $m$ features  $F = \{f_1, f_2, \cdots, f_M\}$.  To obtain an IR-like measure in DS, the first intuition is do the following:

\begin{enumerate}
\item   Replace the notion of a  \emph{document}  with the notion of  \emph{feature}, 
\item  Replace the notion of `a term being in a document'  with the notion of `a term occurring $k$ words close to a feature'.  In other words,  the  simple co-occurrence concept `within document' is replaced with the window-based co-occurrence concept of  `$k$-closed to feature'. 
\end{enumerate}

This provide  us with a Term Frequency- Inverse Feature Term Frequency  (TF-IFTF) formula (word term in the second part is to emphasise that features are words),   as follows:

\begin{definition}
A feature-based  \emph{TF-IFTF}  measure, where \emph{IFTF} stands for `\emph{Inverse Feature Term Frequency}', is defined as follows:

\begin{equation}
\label{tf-iftf}
\TF(t,f) \cdot \textmd{IFTF}(t, F) = freq(t, f) \cdot \log{{M} \over {\mid \{f \in F \mid t  \ k\textmd{close-to} \ f \} \mid}}
\end{equation}
where we have
\begin{itemize}
\item $freq(t, f)$ is the frequency of times term $t$ occurred $k$ words close to feature $f$.
\item $\mid \{f \in F \mid t  \ k\textmd{close-to} \ f \} \mid$ is the total number of features to which term $t$ occurred $k$ words close to. 
\end{itemize}

\end{definition}


Dualizing the  probability notation to  features, we get:

\begin{itemize}
\item $N_F$: total number of features in $F$, i.e. $\mid F \mid$
\item  $n_F (t)$: number of features $k$- close to which term $t$ occurs
\item  $n_L(f)$: number of locations $k$-close to feature $f$
\end{itemize}

we obtain a probabilistic version of TF-IFF as follows

\[
freq(t,f) \cdot \log{\frac{1}{P(t)}}
\]
where   $P(t)$ is the probability of picking a feature with $t$ occurring $k$-close to it, that is
\[
P(t) = \frac{n_F(t)}{N_F}
\]
looking at it location wise, we still have the same formula as for TF-IDF, that is $\frac{n_L(t)}{N_L}$, but here a `location'  is a `feature'.  Then it seems that this does not depend on the feature and is constant. 

To solve the problem,  extend probability notation to
\begin{itemize}
\item $N_{TT}$: total number of target terms
\item  $n_{TT}(f)$: number of target terms which are $k$-close to feature $f$.
\end{itemize}

Define Term Frequency-Inverse Target Term Frequency (TF-ITTF) as follows:

\begin{definition}
\label{TF-ITTF}
\TF-\textmd{ITTF}
\begin{equation}
\label{tf-ittf}
\TF(t,f) \cdot \textmd{ITTF}(t, TT) = freq(t, f) \cdot \log{\frac{1}{P(f)}}
\end{equation}
where
\[
P(f) =  \frac{n_{TT}(f)}{N_{TT}}
\]
\end{definition}
Here, $P(f)$ is the  probability of picking a target term  occurring $k$-close to feature $f$.  This is dual to the InvTermFreq(doc)
used in IR as well, in the context of machine translation



\subsection{TermFreq-InvDocFrec versus FeatureTermFreq-InvTargetTermFreq}

TF can also be dualized from documents to features. In IR, it is `within document term frequency', for which we can also use a location notation as follow
\[
\textmd{freq}(t,d) = n_L(t,d)
\]
Dualizing this to features, we obtain `$k$-close to feature, target  term frequency', as follows:
\begin{equation}
\label{t-f}
\textmd{freq}(t,f) = n_L(t,f)
\end{equation}
This is the number of times term $t$ occurrs $k$-close to feature $f$.  Using reasoning from previous subsection, we know that one can also have a notion of `$k$-close to target term, feature frequency', as follows:
\begin{equation}
\label{f-t}
\textmd{freq}(f,t) = n_L(f,t)
\end{equation}
This is the number of times a feature $f$ occurs $k$-close to a target term $t$.  Evidently, equations \ref{t-f} and \ref{f-t} amount to the same thing. The number of times a target term occurred $k$-close to a feature is the same as the number of times a feature occurs $k$-close to a target term. 
\[
\textmd{freq}(t,f) = \textmd{freq}(f,t) 
\]
Thus the equation \ref{tf-ittf} of definition \ref{TF-ITTF} becomes equal to the following: 
\[
\textmd{FF}(t,f) \cdot \textmd{ITTF}(t, TT) =  freq(f, t) \cdot \log{\frac{1}{P(f)}}
\]
which leads us to  the notion of  {\bf Feature Term Frequency-Inv Target Term Frequency} (FF.ITTF).

\subsection{PMI and TF-IDF}

The PMI is a favourable feature value for the vector-based representation
of a term.
The PMI is also the basis of TF-IDF, and we show-case the relationship
in the equations to follow.

Let $d$ be a document, $q$ a query, and $t$ a term.
The logarithm of likelihood ratio of document and query is:
\[
\log\frac{P(d|q)}{P(d)} =
	\sum_t n_L(t,d) \cdot \log\frac{P(t|q)}{P(t)}
\]
Here, $n_L(t,d)=\textmd{freq}(t,d)$ is the within-document term frequency
quantification.

The likelihood ratio is a measure of (in)dependence.
The decomposition on the right side leads to the measure of independence
between term and query.
\[
\frac{P(t|q)}{P(t)} = \frac{P(t,q)}{P(t) \cdot P(q)} = \frac{P(q|t)}{P(q)}
\]
For NLP, let $t_i$ and $t_j$ be two target terms
(for which we want to establish a similarity).
$t_i$ is the ``document" and ``$t_j$" is the query.
Then:
\[
\log\frac{P(t_i|t_j)}{P(t_i)} =
	\sum_f \textmd{freq}(f,t_i) \cdot \log\frac{P(f|t_j)}{P(f)}
\]
The decomposition on the right side ...:
\[
\frac{P(f|t_j)}{P(f)} =
\frac{P(\textmd{feature}|\textmd{target})}{P(\textmd{feature})} =
\frac{P(\textmd{target}|\textmd{feature})}{P(\textmd{target})}
\]

The problem is that the term probability (the feature probability)
is based on the ``total" occurrence, whereas the IDF is based
on counting documents.
In other words,
the feature probability (for IR, term probability) is based on
the occurrence of the event:
$P(\textmd{feature term}):=P_L(\textmd{feature term})$
is based on the Locations (the number of times the feature occurs), so is
$P(t):=P_L(t)$.

Transformation to the ``document" (the target term) via Poisson bridge.
...

For IR:
\[
P(t) = P(t|q) \cdot P_D(t)
\]
\[
P(t|q) = \avgtf(t) / \avgdl
\]
Therefore:
\[
\log\frac{P(t|q)}{P(t)} = \log\frac{1}{P_D(t)}
\]

For NLP/PMI:
\[
P(f) = P(f|t) \cdot P_\textmd{TargetTerms}(f)
\]
\[
\log\frac{P(f|t_j)}{P(f)} = \log\frac{1}{P_\textmd{TargetTerms}(f)}
\]
Expresses how discriminative the feature is.


FORM INNER PRODUCT OF TWO TARGET TERMS


\section{Phrases}

Title: Phrase-based


In CDS


In IR


\subsection{XF-IDF: Scoring Subject-Verb-Object Phrases}

The main idea of XF-IDF \cite{Azzam/etal:SIGMOD:KEYS:2010} is to extend
TF-IDF towards a ranking model that supports phrases.
%
A document is retrieved if it contains at least one of the query phrases.
%
The impact of a phrase on the retrieval score is given by the XF-IDF
weight of a phrase.
%
Let $(s,v,o)$ be a subj-verb-obj phrase.
In more general, this can be a phrase of any length with any structure
(e.g.~subject, relationship, object), for the for purpose of this paper,
we focus the discussion on s-v-o phrases.
%
Let $\textmd{type}(x,(s,v,o))$ be a function that retuns the type of the component
of a phrase, i.e.~$\textmd{type}(s,(s,v,o))=\textmd{subject}$.

Then:
\[
w_\textmd{XF-IDF}((s,v,o)) :=
\sum_{x \in \{s,v,o\}} \TF(x,d) \cdot \IDF(x) \cdot P(\textmd{type}(x,(s,v,o)))
\]
To illustrate, consider the XF-IDF weight of the phrase ``woman likes sport".

...

\begin{eqnarray}
\label{tf-idf-phrase}
\RSV_\textmd{TF-IDF}(d,q) &:=& \sum_{t \in q} \textmd{score}(t,d,q)\\
\textmd{score}(t,d,q) &:=&
\TF(t,d) \cdot \IDF(t)
\end{eqnarray}





\section{Why the independence measure is best}

Multiplication of three vectors: s, v and o.
\[
\frac{
P(c|s) \cdot P(c|v) \cdot P(c|o)}{
P(c) \cdot P(c) \cdot P(c)} =
\frac{
P(s|c) \cdot P(v|c) \cdot P(o|c)
}{
P(s) \cdot P(v) \cdot P(o)
}
\]
Let $t_i$ and $t_j$ be two propositions
(e.g.~s-v-o).

$\textmd{sim}(t_i,t_j)$ is high if
\begin{enumerate}
\item many features occur within the context of subject, verb, and object
\item the features are rare
\end{enumerate}

cos: corresponds to sum over independence measure.
Therefore, pmi is the correct model.
Corresponds to
\[
\sum_c \log(...) = \log \prod_c(...)
\]



Does not work for different
spaces for nouns and verbs?

The multiplication yields a probability
for the subj-verb-obj sequence where all
components are independent of each other.

Multiplication of two conditional probs means then what?
Effect of L2 norm?


Relationship to LM - there should be an obvious one.


\section{Decomposing Similarity}
Decompsoed additive baseline similarity
\[
\frac{cos(sbj1,sbj2) + cos(vrb1,vrb2)) + cos(obj1,obj2)}{3}
\]

Can any of the NLP sentence similarities decompose? It does not seem so, for example
\begin{eqnarray*}
cos(sbj1+verb1,sbj2+vrb2) &\neq& cos(sbj1, sbj2) + cos(vrb1,vrb2) \\
cos(sbj1\odot verb1,sbj2+vrb2) &\neq& cos(sbj1, sbj2) \odot cos(vrb1,vrb2)
\end{eqnarray*}
For kronecker composition has to move between vector of different tensor rank, i.e. either from $V$ to $V \otimes V$ or from the latter to the former. Diagonalization might be helpful here, also convolution kernel might help. 

In general, may be the sentence cosines, do doecompsoe but not in a direct way as above, that is when left and right have the same operation. The operations might vary from lef to right, or the ratios might be preserved etc. 



\section{cosines again}

\begin{eqnarray}
\cos(angle(s\odot v\odot o, ...) &=&  1/ \sum P(s,v,o|c) \cdot P(c)\\
\cos(angle(s+v+o, ...) &=& \sum (P(s|c)  \cdot  ... + P(v|c) ... + ....)  \cdot  ....\\
\cos(angle(s+v+o, ...) &=&  \sum (\log{\Pi_{x in s,v,o} (P(x,c)/P(x) \cdot P(c)) ...}\\
\end{eqnarray}

\section{various log measures}

\begin{eqnarray}
\textmd{NLogP}&   N(t,c)  \cdot \log{1}{P(c)}\\
\textmd{LogNLogP}&  log(N(t,c)+1) \cdot \log{1}{P(c)}\\
\textmd{PExpN}&    P(c)^{-N(t,c)}\\
\textmd{PExpLogN}& P(c)^{-\log{N(t,c)+1}}\\
\end{eqnarray}

\begin{table*}[htbp]
\begin{tabularx}{\textwidth}{l|XX}
& NLP & IR\\
\hline
co-occurrence
&
        co-occurrence between the semantic symbol (target word) and feature words
        &
        co-occurrence between the semantic symbols (words) themselves
        \\
        \hline
representation of words
        & distributional & symbolic\\
        \hline
single vs set & similarity between two
    \emph{single phrases}
    & relationship (implication, entailment) between two
    \emph{sets of phrases}\\

symmetric yes/no &
    similarity is a symmetric function &
relationship between sets is not symmetric; moreover, the phrase-based score is not necessarily symmetric\\
\hline
similar/relevant
& phrase $t_i$ is similar to phrase $t_j$ &
document~$d$ (source) is relevant with respect to query~$q$ (target)\\
\hline
scores
& the similarity score is estimated
based on the distance/angle between distributional vectors
& the relevance score is estimated
based on the retrieval model that computes the implication between the set of document and the set of query propositions\\
probabilistic semantics
& (in)dependence between target word and feature word: $\frac{P(w_t,w_f)}{P(w_t) \cdot P(w_f)}$
& (in)dependent between document and query:
$\frac{P(d,q)}{P(d) \cdot P(q)}$\\
\hline
& for this work: virtual query has exactly one proposition; virtual document has exactly one condition; therefore, the similarity score $\textmd{sim}(\textmd{phrase1},\textmd{phrase2})$ can be compared to the retrieval score
$\textmd{RSV}(\textmd{document: set of phrases},\textmd{query: set of phrases})$.

TODO: how to get this over two columns
\\
        \hline
\end{tabularx}
\end{table*}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
